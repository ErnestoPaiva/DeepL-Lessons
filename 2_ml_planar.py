# -*- coding: utf-8 -*-
"""2 Curso ML - Planar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MuFSVtCyRYVyx1AGmbsVwSfONOZqAkz8
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras import Sequential
from tensorflow.keras.activations import sigmoid, tanh

import sklearn
import sklearn.datasets

def load_planar_dataset():
    np.random.seed(1)
    m = 400 # number of examples
    N = int(m/2) # number of points per class
    D = 2 # dimensionality
    X = np.zeros((m,D)) # data matrix where each row is a single example
    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)
    a = 4 # maximum ray of the flower

    for j in range(2):
        ix = range(N*j,N*(j+1))
        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta
        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius
        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]
        Y[ix] = j
        
    #X = X.T
    #Y = Y.T

    return X, Y

def plot_decision_boundary(model, X, y):
    # Set min and max values and give it some padding
    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1
    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole grid
    Z = model(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.ylabel('x2')
    plt.xlabel('x1')
    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)

def load_extra_datasets():  
    N = 200
    noisy_circles = sklearn.datasets.make_circles(n_samples=N, factor=.5, noise=.3)
    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=.2)
    blobs = sklearn.datasets.make_blobs(n_samples=N, random_state=5, n_features=2, centers=6)
    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(mean=None, cov=0.5, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None)
    no_structure = np.random.rand(N, 2), np.random.rand(N, 2)
    
    return noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure

"""# Ejemplo 1"""

x, y = load_planar_dataset()
# Dimensiones de los conjuntos
print ("x shape: " + str(x.shape))
print ("y shape: " + str(y.shape))

print('Cantidad de ejemplos de clase 0:', np.sum(y==0))
print('Cantidad de ejemplos de clase 1:', np.sum(y==1))

import matplotlib.pyplot as plt
_ = plt.hist(x, bins='auto')  # arguments are passed to np.histogram
plt.title("Histogram with 'auto' bins")
plt.show()

plt.scatter(x[:, 0], x[:, 1], c=y, s=40, cmap=plt.cm.Spectral);

x.shape[-1]

# Creación del modelo
model = Sequential([
                    Dense(units=1, input_shape= [x.shape[-1]], activation=tanh),
                    Dense(units=1, activation=sigmoid)
                    ])

# Arquitectura del modelo
model.summary()

# Definición del optimizador, función de pérdidas y métricas
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss= tf.keras.losses.BinaryCrossentropy(),metrics=tf.keras.metrics.BinaryAccuracy() )

model.fit(x, y, epochs=1000, batch_size=400)

plot_decision_boundary(lambda x: model.predict_classes(x), x.T, y.T)

# This may take about 2 minutes to run

plt.figure(figsize=(16, 32))
hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]
for i, n_h in enumerate(hidden_layer_sizes):
    plt.subplot(5, 2, i+1)
    plt.title('Hidden Layer of size %d' % n_h)
    model = Sequential([
                    Dense(units=n_h, input_shape= [x.shape[-1]], activation=tanh),
                    Dense(units=1, activation=sigmoid)
                    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss= tf.keras.losses.BinaryCrossentropy(),metrics=tf.keras.metrics.BinaryAccuracy() )
    model.fit(x, y, epochs=1000, batch_size=400, verbose= 0)
    plot_decision_boundary(lambda x: model.predict_classes(x), x.T, y.T)
    predictions = model.predict_classes(x)
    accuracy = float((np.dot(y.T,predictions) + np.dot(1-y.T,1-predictions))/float(y.size)*100)
    print ("Accuracy for {} hidden units: {} %".format(n_h, accuracy))

"""# Ejemplo 2"""

# Datasets
noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()

datasets = {"noisy_circles": noisy_circles,
            "noisy_moons": noisy_moons,
            "blobs": blobs,
            "gaussian_quantiles": gaussian_quantiles}

### START CODE HERE ### (choose your dataset)
dataset = "noisy_moons"
### END CODE HERE ###

x, y = datasets[dataset]
#X, Y = X.T, Y.reshape(1, Y.shape[0])

# make blobs binary
if dataset == "blobs":
    y = y%2

# Visualize the data
plt.scatter(x[:, 0], x[:, 1], c=y, s=40, cmap=plt.cm.Spectral);

_ = plt.hist(x, bins='auto')  # arguments are passed to np.histogram
plt.title("Histogram with 'auto' bins")
plt.show()

# Dimensiones de los conjuntos
print ("x shape: " + str(x.shape))
print ("y shape: " + str(y.shape))

# Creación del modelo
model = Sequential([
                    Dense(units=1, input_shape= [x.shape[-1]], activation=tanh),
                    Dense(units=1, activation=sigmoid)
                    ])
model.summary()

# Definición del optimizador, función de pérdidas y métricas
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss= tf.keras.losses.BinaryCrossentropy(),metrics=tf.keras.metrics.BinaryAccuracy() )
model.fit(x, y, epochs=1000, batch_size=200)

plot_decision_boundary(lambda x: model.predict_classes(x), x.T, y.T)

# This may take about 2 minutes to run

plt.figure(figsize=(16, 32))
hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]
for i, n_h in enumerate(hidden_layer_sizes):
    plt.subplot(5, 2, i+1)
    plt.title('Hidden Layer of size %d' % n_h)
    model = Sequential([
                    Dense(units=n_h, input_shape= [x.shape[-1]], activation=tanh),
                    Dense(units=1, activation=sigmoid)
                    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss= tf.keras.losses.BinaryCrossentropy(),metrics=tf.keras.metrics.BinaryAccuracy() )
    model.fit(x, y, epochs=1000, batch_size=400, verbose= 0)
    plot_decision_boundary(lambda x: model.predict_classes(x), x.T, y.T)
    predictions = model.predict_classes(x)
    accuracy = float((np.dot(y.T,predictions) + np.dot(1-y.T,1-predictions))/float(y.size)*100)
    print ("Accuracy for {} hidden units: {} %".format(n_h, accuracy))

