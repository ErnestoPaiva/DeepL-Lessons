# -*- coding: utf-8 -*-
"""5 Curso ML - MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OQlPiKLoNAFDIJOcMyNak544n89_Q3aX
"""

import numpy as np
import matplotlib.pyplot as plt
import h5py

import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras import Sequential
from tensorflow.keras.activations import sigmoid, softmax, relu

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2

def figure(History, legend):
  ####################### PLOT TRAINING VS VALIDATION ######################
  ########## Accuracy ###########
  acc = History.history['acc']
  val_acc = History.history['val_acc']
  loss = History.history['loss']
  val_loss = History.history['val_loss']

  plt.plot(acc)
  plt.plot(val_acc)
  plt.title('model accuracy')
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(legend, loc='upper left')
  plt.grid()
  plt.show()

  ########## Loss ###########
  plt.plot(loss)
  plt.plot(val_loss)
  plt.title('model loss')
  plt.ylabel('loss')
  plt.xlabel('epoch')
  try:
    loss_no_reg = History.history['categorical_crossentropy']
    val_loss_no_reg = History.history['val_categorical_crossentropy']
    plt.plot(loss_no_reg)
    plt.plot(val_loss_no_reg)
    plt.legend(legend + [legend[0]+' sin reg', legend[1]+' sin reg'], loc='upper left')
  except:
    plt.legend(legend, loc='upper left')

  plt.grid()
  plt.show()

# load dataset
(TRAIN_x_orig, TRAIN_y), (test_x_orig, test_y) = mnist.load_data()

# Dimensiones de los conjuntos de ENTRENAMIENTO y prueba
print ("TRAIN_x_orig shape: " + str(TRAIN_x_orig.shape))
print ("TRAIN_y shape: " + str(TRAIN_y.shape))
print ("test_x_orig shape: " + str(test_x_orig.shape))
print ("test_y shape: " + str(test_y.shape))

# Ejemplo de una imagen
index = 3
plt.imshow(TRAIN_x_orig[index], cmap='gray')
plt.title(TRAIN_y[index])

# Aplanamiento de la imagen
TRAIN_x_flatten = TRAIN_x_orig.reshape(TRAIN_x_orig.shape[0], -1)
test_x_flatten =  test_x_orig.reshape(test_x_orig.shape[0], -1)

print ("TRAIN_x_flatten shape: " + str(TRAIN_x_flatten.shape))
print ("TRAIN_y shape: " + str(TRAIN_y.shape))
print ("test_x_flatten shape: " + str(test_x_flatten.shape))
print ("test_y shape: " + str(test_y.shape))

for i in range(10):
  print(i,' :', np.sum(TRAIN_y==i))

# Escalamiento de los datos
TRAIN_x = TRAIN_x_flatten/255.
test_x = test_x_flatten/255.

# Segmentación en entrenamiento y validación
train_x, valid_x, train_y, valid_y = train_test_split(TRAIN_x, TRAIN_y, test_size=1/6, random_state=42)

# Resumen de dimensiones
print ("TRAIN_x shape: " + str(TRAIN_x.shape))
print ("TRAIN_y shape: " + str(TRAIN_y.shape))
print ("train_x shape: " + str(train_x.shape))
print ("train_y shape: " + str(train_y.shape))
print ("valid_x shape: " + str(valid_x.shape))
print ("valid_y shape: " + str(valid_y.shape))
print ("test_x shape:  " + str(test_x.shape))
print ("test_y shape:  " + str(test_y.shape))

test_y

#Converts a class vector
TRAIN_y_cat = to_categorical(TRAIN_y)
train_y_cat = to_categorical(train_y)
valid_y_cat = to_categorical(valid_y)
test_y_cat = to_categorical(test_y)

print ("TRAIN_y_cat shape: " + str(TRAIN_y_cat.shape))
print ("train_y_cat shape: " + str(train_y_cat.shape))
print ("valid_y_cat shape: " + str(valid_y_cat.shape))
print ("test_y_cat shape:  " + str(test_y_cat.shape))

"""# Modelo simple"""

# Creación del modelo
model = Sequential([
                    Dense(units=10, input_shape= [train_x.shape[-1]], activation=softmax)
                    ])

# Arquitectura del modelo
model.summary()

# Definición del optimizador, función de pérdidas y métricas
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002), loss= tf.keras.losses.CategoricalCrossentropy(),metrics='acc')
# Ajuste de pesos
History = model.fit(train_x, train_y_cat, epochs=30, batch_size=32, validation_data= (valid_x, valid_y_cat))

# Plot de curvas
figure(History,['Train', 'Valid'])

# Metricas
Metricas = model.evaluate(train_x, train_y_cat , verbose=0)
print('train Loss: %.4f, train Acc: %.4f' % (Metricas[0], Metricas[1]) )
Metricas = model.evaluate(valid_x, valid_y_cat , verbose=0)
print('valid Loss: %.4f, valid Acc: %.4f' % (Metricas[0], Metricas[1]) )

"""# Modelo complejo"""

# Creación del modelo
model = Sequential([
                    Dense(units=200, input_shape= [train_x.shape[-1]], activation=relu),
                    Dense(units=50, activation=relu),
                    Dense(units=10, activation=softmax)
                    ])

# Arquitectura del modelo
model.summary()

# Definición del optimizador, función de pérdidas y métricas
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0004), loss= tf.keras.losses.CategoricalCrossentropy(),metrics='acc')
# Ajuste de pesos
History = model.fit(train_x, train_y_cat, epochs=30, batch_size=32, validation_data= (valid_x, valid_y_cat))

# Plot de curvas
figure(History,['Train', 'Valid'])

# Metricas
Metricas = model.evaluate(train_x, train_y_cat , verbose=0)
print('train Loss: %.4f, train Acc: %.4f' % (Metricas[0], Metricas[1]) )
Metricas = model.evaluate(valid_x, valid_y_cat , verbose=0)
print('valid Loss: %.4f, valid Acc: %.4f' % (Metricas[0], Metricas[1]) )

for i in range(len(model.get_weights())):
  _ = plt.hist(model.get_weights()[i].reshape((-1,)), bins='auto')
  if i%2==0:
    plt.title('w, ' + 'Layer ' + str(i//2+1))
  else:
    plt.title('b, ' + 'Layer ' + str(i//2+1))

  plt.show()

"""# Dropout"""

# Creación del modelo
model = Sequential([
                    Dense(units=200, input_shape= [train_x.shape[-1]], activation=relu),
                    Dropout(0.35),
                    Dense(units=50, activation=relu),
                    Dropout(0.35),
                    Dense(units=10, activation=softmax)
                    ])

# Arquitectura del modelo
model.summary()

# Definición del optimizador, función de pérdidas y métricas
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss= tf.keras.losses.CategoricalCrossentropy(),metrics='acc')
# Ajuste de pesos
History = model.fit(train_x, train_y_cat, epochs=30, batch_size=32, validation_data= (valid_x, valid_y_cat))

# Plot de curvas
figure(History,['Train', 'Valid'])

# Metricas
Metricas = model.evaluate(train_x, train_y_cat , verbose=0)
print('train Loss: %.4f, train Acc: %.4f' % (Metricas[0], Metricas[1]) )
Metricas = model.evaluate(valid_x, valid_y_cat , verbose=0)
print('valid Loss: %.4f, valid Acc: %.4f' % (Metricas[0], Metricas[1]) )

"""# Early stopping"""

# Creación del modelo
model = Sequential([
                    Dense(units=200, input_shape= [train_x.shape[-1]], activation=relu),
                    Dense(units=50, activation=relu),
                    Dense(units=10, activation=softmax)
                    ])

# Arquitectura del modelo
model.summary()

# Definición del optimizador, función de pérdidas y métricas
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss= tf.keras.losses.CategoricalCrossentropy(),metrics='acc')
# Early stopping
stopping = EarlyStopping(monitor='val_loss',min_delta=0,mode='auto',patience=2, restore_best_weights=True)
# Ajuste de pesos
History = model.fit(train_x, train_y_cat, epochs=30, batch_size=32, callbacks=[stopping], validation_data= (valid_x, valid_y_cat))

figure(History,['train', 'valid'])

# Metricas
Metricas = model.evaluate(train_x, train_y_cat , verbose=0)
print('train Loss: %.4f, train Acc: %.4f' % (Metricas[0], Metricas[1]) )
Metricas = model.evaluate(valid_x, valid_y_cat , verbose=0)
print('valid Loss: %.4f, valid Acc: %.4f' % (Metricas[0], Metricas[1]) )

"""# weight regularizers"""

# Creación del modelo
model = Sequential([
                    Dense(units=200, input_shape= [train_x.shape[-1]], activation=relu, kernel_regularizer= l2(0.0001)  ),
                    Dense(units=50, activation=relu, kernel_regularizer= l2(0.0001)),
                    Dense(units=10, activation=softmax, kernel_regularizer= l2(0.0001)),
                    ])

# Arquitectura del modelo
model.summary()

# Definición del optimizador, función de pérdidas y métricas
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss= tf.keras.losses.CategoricalCrossentropy(),metrics=['acc', tf.keras.losses.CategoricalCrossentropy()])
# Ajuste de pesos
History = model.fit(train_x, train_y_cat, epochs=30, batch_size=32, validation_data= (valid_x, valid_y_cat))

figure(History,['train', 'valid'])

# Metricas
Metricas = model.evaluate(train_x, train_y_cat , verbose=0)
print('train Loss: %.4f, train Acc: %.4f' % (Metricas[2], Metricas[1]) )
Metricas = model.evaluate(valid_x, valid_y_cat , verbose=0)
print('valid Loss: %.4f, valid Acc: %.4f' % (Metricas[2], Metricas[1]) )

for i in range(len(model.get_weights())):
  _ = plt.hist(model.get_weights()[i].reshape((-1,)), bins='auto', density = True)
  plt.ylim((0,10))
  if i%2==0:
    plt.title('w, ' + 'Layer ' + str(i//2+1))
  else:
    plt.title('b, ' + 'Layer ' + str(i//2+1))
  plt.grid()
  plt.show()

"""# Uniendo todos los métodos"""

# Creación del modelo
model = Sequential([
                    Dense(units=200, input_shape= [train_x.shape[-1]], activation=relu, kernel_regularizer= l2(0.0001)  ),
                    Dropout(0.35),
                    Dense(units=50, activation=relu, kernel_regularizer= l2(0.0001)  ),
                    Dropout(0.35),
                    Dense(units=10, activation=softmax, kernel_regularizer= l2(0.0001)  ),
                    ])

# Arquitectura del modelo
model.summary()

# Definición del optimizador, función de pérdidas y métricas
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss= tf.keras.losses.CategoricalCrossentropy(),metrics=['acc', tf.keras.losses.CategoricalCrossentropy()])
# Early stopping
stopping = EarlyStopping(monitor='val_categorical_crossentropy',min_delta=0,mode='auto',patience=10, restore_best_weights=True)
# Ajuste de pesos
History = model.fit(train_x, train_y_cat, epochs=50, batch_size=32, callbacks=[stopping], validation_data= (valid_x, valid_y_cat))

figure(History,['train', 'valid'])

# Metricas
Metricas = model.evaluate(train_x, train_y_cat , verbose=0)
print('train Loss: %.4f, train Acc: %.4f' % (Metricas[2], Metricas[1]) )
Metricas = model.evaluate(valid_x, valid_y_cat , verbose=0)
print('valid Loss: %.4f, valid Acc: %.4f' % (Metricas[2], Metricas[1]) )

"""# Test"""

# Creación del modelo
modelo_final = Sequential([
                    Dense(units=200, input_shape= [TRAIN_x.shape[-1]], activation=relu, kernel_regularizer= l2(0.0001)  ),
                    Dropout(0.35),
                    Dense(units=50, activation=relu, kernel_regularizer= l2(0.0001)   ),
                    Dropout(0.35),
                    Dense(units=10, activation=softmax, kernel_regularizer= l2(0.0001)    ),
                    ])

# Arquitectura del modelo
modelo_final.summary()

# Definición del optimizador, función de pérdidas y métricas
modelo_final.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss= tf.keras.losses.CategoricalCrossentropy(),metrics=['acc', tf.keras.losses.CategoricalCrossentropy()])
# Early stopping
stopping = EarlyStopping(monitor='val_categorical_crossentropy',min_delta=0,mode='auto',patience=10, restore_best_weights=True)
# Ajuste de pesos
History = modelo_final.fit(TRAIN_x, TRAIN_y_cat, epochs=50, batch_size=32, callbacks=[stopping], validation_data= (test_x, test_y_cat))

figure(History,['TRAIN', 'test'])

# Metricas
Metricas = modelo_final.evaluate(TRAIN_x, TRAIN_y_cat , verbose=0)
print('TRAIN Loss: %.4f, TRAIN Acc: %.4f' % (Metricas[2], Metricas[1]) )
Metricas = modelo_final.evaluate(test_x, test_y_cat , verbose=0)
print('test Loss: %.4f, test Acc: %.4f' % (Metricas[2], Metricas[1]) )

for i in range(len(modelo_final.get_weights())):
  _ = plt.hist(modelo_final.get_weights()[i].reshape((-1,)), bins='auto', density = True)
  plt.ylim((0,10))
  if i%2==0:
    plt.title('w, ' + 'Layer ' + str(i//2+1))
  else:
    plt.title('b, ' + 'Layer ' + str(i//2+1))
  plt.grid()
  plt.show()







